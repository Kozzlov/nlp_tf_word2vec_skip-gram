{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvmhnAGv2ObtmLM5BbdkjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kozzlov/nlp_tf_word2vec_skip-gram/blob/main/Word2Vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrmfdoRqNUWK"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqQP6IkRGqD5",
        "outputId": "b82a7e0a-0087-4f85-95f7-1a36c86d0b30"
      },
      "source": [
        "!pip install -q tqdm\r\n",
        "\r\n",
        "import io \r\n",
        "import itertools\r\n",
        "import numpy as np \r\n",
        "import os \r\n",
        "import re\r\n",
        "import string \r\n",
        "import tensorflow as tf\r\n",
        "import tqdm\r\n",
        "\r\n",
        "from tensorflow.keras import Model, Sequential \r\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "\r\n",
        "SEED = 42 \r\n",
        "AUTOTUNE = tf.data.AUTOTUNE\r\n",
        "\r\n",
        "#Consider the following sentence\r\n",
        "sentence = \"The wide road shimmered in the hot sun\"\r\n",
        "tokens = list (sentence.lower().split())\r\n",
        "print(len(tokens))\r\n",
        "#Create a vocabulary to save mappings from tokens to integer indices\r\n",
        "vocab, index = {}, 1 #start index form 1\r\n",
        "vocab['<pad>'] = 0 # add a padding token \r\n",
        "for token in tokens:\r\n",
        "  if token not in vocab:\r\n",
        "    vocab[token] = index \r\n",
        "    index += 1\r\n",
        "vocab_size = len(vocab)\r\n",
        "print(vocab)\r\n",
        "#Create an inverse vocabulary to save mappings from integer indices to tokens\r\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\r\n",
        "print(inverse_vocab)\r\n",
        "#Vectorize the sentense \r\n",
        "example_sequence = [vocab[word] for word in tokens]\r\n",
        "print(example_sequence)\r\n",
        "#(Generate skip-grams)\r\n",
        "#Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word\r\n",
        "window_size = 2\r\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
        "    example_sequence, \r\n",
        "    vocabulary_size=vocab_size, \r\n",
        "    window_size=window_size,\r\n",
        "    negative_samples=0)\r\n",
        "print(len(positive_skip_grams))\r\n",
        "\r\n",
        "#for target, context in positive_skip_grams[:5]:\r\n",
        "#print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\r\n",
        "\r\n",
        "#Negative sampling for one skip-gram\r\n",
        "#Get target and context words for one positive skip-gram\r\n",
        "target_word, context_word = positive_skip_grams[0]\r\n",
        "#Set the number of negative samples per positive context\r\n",
        "num_ns = 4\r\n",
        "\r\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\r\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
        "    true_classes=context_class,#classes that sould be sampled as positive\r\n",
        "    num_true=1,#each posistive skip-gram has one positive context class \r\n",
        "    num_sampled=num_ns,#number of negative context words to sample\r\n",
        "    unique=True,#all the negative samples must be unique\r\n",
        "    range_max=vocab_size,#index from the sample, from [0, vocab_size]\r\n",
        "    seed=SEED,#seed for reproducibility\r\n",
        "    name=\"negative_sampling\"#name of this operation\r\n",
        ")\r\n",
        "\r\n",
        "print(negative_sampling_candidates)\r\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\r\n",
        "\r\n",
        "#creating a training example \r\n",
        "#adding a dimention so to use concatination (on the next step)\r\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\r\n",
        "#concatting positive and negative sampled words\r\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
        "#label first context word as 1 (positive) followed ny num_ns 0s (negative)\r\n",
        "label=tf.constant([1]+ [0]*num_ns, dtype=\"int64\")\r\n",
        "#reshaping target to shape (1,) and context label to (num_ns+1,)\r\n",
        "target=tf.squeeze(target_word)\r\n",
        "context=tf.squeeze(context)\r\n",
        "label=tf.squeeze(label)\r\n",
        "\r\n",
        "# print(f\"target_index    : {target}\")\r\n",
        "# print(f\"target_word     : {inverse_vocab[target_word]}\")\r\n",
        "# print(f\"context_indices : {context}\")\r\n",
        "# print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\r\n",
        "# print(f\"label           : {label}\")\r\n",
        "\r\n",
        "# print(f\"target  :\", target)\r\n",
        "# print(f\"context :\", context )\r\n",
        "# print(f\"label   :\", label )\r\n",
        "\r\n",
        "#skip-gram sampling table \r\n",
        "sampling_table=tf.keras.preprocessing.sequence.make_sampling_table(size=10)\r\n",
        "print(sampling_table)\r\n",
        " \r\n",
        "#generating training table\r\n",
        "#generate skip-gram pairs with negative sampling for the list of sequences\r\n",
        "#(int-encoded sentences) based on window size, number of negative samples\r\n",
        "#and vocabulary size\r\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\r\n",
        "  #elements of each training example are appended to these lists\r\n",
        "  targets, contexts, labels = [], [], []\r\n",
        "\r\n",
        "  #building the sampling table for vacab_size tokens\r\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\r\n",
        "\r\n",
        "  #iterate over all sequences in dataset \r\n",
        "  for sequence in tqdm.tqdm(sequences):\r\n",
        "    #generate positive skip-gram pairs for a sequence\r\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
        "        sequence,\r\n",
        "        vocabulary_size=vocab_size,\r\n",
        "        sampling_table=sampling_table,\r\n",
        "        window_size=window_size,\r\n",
        "        negative_samples=0)\r\n",
        "    for target_word, context_word in positive_skip_grams:\r\n",
        "      context_class = tf.expand_dims(\r\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\r\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
        "          true_classes=context_class,\r\n",
        "          num_true=1,\r\n",
        "          num_sampled=num_ns,\r\n",
        "          unique=True,\r\n",
        "          range_max=vocab_size,\r\n",
        "          seed=SEED,\r\n",
        "          name=\"negative_sampling\"\r\n",
        "      )\r\n",
        "      #building context and lable vectors (for one target word)\r\n",
        "      negative_sampling_candidates =tf.expand_dims(\r\n",
        "          negative_sampling_candidates, 1)\r\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
        "      \r\n",
        "      #append each element from the training example to the global lists \r\n",
        "      targets.append(target_word)\r\n",
        "      contexts.append(context)\r\n",
        "      labels.append(label)\r\n",
        "\r\n",
        "  return targets, contexts, labels\r\n",
        "\r\n",
        "#Preparing training data for Word2Vec\r\n",
        "#Downloading text corpus\r\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\r\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n",
        "# with open(path_to_file) as f:\r\n",
        "#   lines = f.read().splitlines()\r\n",
        "# for line in lines[:20]:\r\n",
        "#   print(line)\r\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\r\n",
        "\r\n",
        "#vectorize sentences from corpus\r\n",
        "#custom standartization function to lowercase text and remove punctuation\r\n",
        "def custom_standardization(input_data):\r\n",
        "  lowercase = tf.strings.lower(input_data)\r\n",
        "  return tf.strings.regex_replace(lowercase,\r\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\r\n",
        "#define the vocabulary size and number of words in a sequence\r\n",
        "vocab_size=4096\r\n",
        "sequence_length=10\r\n",
        "#use the text vectorization layer to normalize, split and map strings to integers\r\n",
        "#set output_sequence_length length to pad all samples to the same length.\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "  standardize=custom_standardization,\r\n",
        "  max_tokens=vocab_size,\r\n",
        "  output_mode='int',\r\n",
        "  output_sequence_length=sequence_length)\r\n",
        "#call adapt to create vocabulary\r\n",
        "vectorize_layer.adapt(text_ds.batch(1024))\r\n",
        "#save the created vocabulary for reference\r\n",
        "inverse_vocab=vectorize_layer.get_vocabulary()\r\n",
        "print(inverse_vocab[:20])\r\n",
        "#now vectorize-layer can be used to generate vectors for each alement in the text_ds\r\n",
        "def vectorize_text(text):\r\n",
        "  text=tf.expand_dims(text, -1)\r\n",
        "  return tf.squeeze(vectorize_layer(text))\r\n",
        "#vectorize the data in text_ds\r\n",
        "text_vector_ds=text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\r\n",
        "\r\n",
        "#obtain sequence from the dataset\r\n",
        "sequences=list(text_vector_ds.as_numpy_iterator())\r\n",
        "print(len(sequences))\r\n",
        "\r\n",
        "# for seq in sequences[:5]:\r\n",
        "#   print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\r\n",
        "\r\n",
        "#generating training examples from the sequence \r\n",
        "targets, contexts, labels = generate_training_data(\r\n",
        "    sequences=sequences,\r\n",
        "    window_size=2,\r\n",
        "    num_ns=4,\r\n",
        "    vocab_size=vocab_size,\r\n",
        "    seed=SEED)\r\n",
        "print(len(targets), len(contexts), len(labels))\r\n",
        "\r\n",
        "#configure the dataset for perfomance \r\n",
        "BATCH_SIZE=1024\r\n",
        "BUFFER_SIZE=10000\r\n",
        "dataset=tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\r\n",
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "print(dataset)\r\n",
        "\r\n",
        "#model and training \r\n",
        "#subclassed Word2Vec model\r\n",
        "class Word2Vec(Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim):\r\n",
        "    super(Word2Vec, self).__init__()\r\n",
        "    self.target_embedding = Embedding(vocab_size,\r\n",
        "                                      embedding_dim,\r\n",
        "                                      input_length=1,\r\n",
        "                                      name=\"w2v_embedding\", )\r\n",
        "    self.context_embedding = Embedding(vocab_size,\r\n",
        "                                       embedding_dim,\r\n",
        "                                       input_length=num_ns+1)\r\n",
        "    self.dots = Dot(axes=(3,2))\r\n",
        "    self.flatten = Flatten()\r\n",
        "  def call(self, pair):\r\n",
        "    target, context = pair\r\n",
        "    we = self.target_embedding(target)\r\n",
        "    ce = self.context_embedding(context)\r\n",
        "    dots = self.dots([ce, we])\r\n",
        "    return self.flatten(dots)\r\n",
        "\r\n",
        "#define loss function and compile model\r\n",
        "embedding_dim = 128\r\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\r\n",
        "word2vec.compile(optimizer='adam',\r\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
        "                 metrics=['accuracy'])\r\n",
        "\r\n",
        "#callback to log training statistics for tensorboard\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\r\n",
        "\r\n",
        "#training the model \r\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\r\n",
        "\r\n",
        "# representing accuracy and loss with Tensorboard for Word2Vec model\r\n",
        "#%tensorboard --logdir logs (CHECK WHY IT DOES NOT WORK)\r\n",
        "\r\n",
        "#embedding lookup and analysis\r\n",
        "weights=word2vec.get_layer('w2v_embedding').get_weights()[0]\r\n",
        "vocab=vectorize_layer.get_vocabulary()\r\n",
        "\r\n",
        "#creating and saving the vectors metadata file\r\n",
        "out_v=io.open('vectors.tsv', 'w', encoding='utf-8')\r\n",
        "out_m=io.open('metadata.tsv', 'w', encoding='utf-8')\r\n",
        "\r\n",
        "for index, word in enumerate(vocab):\r\n",
        "  if index == 0: continue #skip 0, it is padding \r\n",
        "  vec=weights[index]\r\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
        "  out_m.write(word + \"\\n\")\r\n",
        "out_v.close()\r\n",
        "out_m.close()\r\n",
        "\r\n",
        "#downloading vectors.tsv/metadata.tsv (in order to analyze embedding in EMBEDDING PROJECTOR)\r\n",
        "# try:\r\n",
        "#   from google.colab import files\r\n",
        "#   files.download('vectors.tsv')\r\n",
        "#   files.download('metadata.tsv')\r\n",
        "# except Exception as e:\r\n",
        "#   pass"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n",
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n",
            "[1, 2, 3, 4, 5, 1, 6, 7]\n",
            "26\n",
            "tf.Tensor([6 4 2 0], shape=(4,), dtype=int64)\n",
            "['hot', 'shimmered', 'wide', '<pad>']\n",
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n",
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 415/32777 [00:00<00:07, 4146.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32777/32777 [00:07<00:00, 4103.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65308 65308 65308\n",
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n",
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 15ms/step - loss: 1.6087 - accuracy: 0.2216\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.5918 - accuracy: 0.5781\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 1.5514 - accuracy: 0.6331\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.4743 - accuracy: 0.5926\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.3764 - accuracy: 0.5903\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.2781 - accuracy: 0.6130\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.1849 - accuracy: 0.6455\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.0981 - accuracy: 0.6790\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.0192 - accuracy: 0.7132\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.9475 - accuracy: 0.7425\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.8781 - accuracy: 0.7682\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.8145 - accuracy: 0.7916\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.7576 - accuracy: 0.8113\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.7059 - accuracy: 0.8279\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.6583 - accuracy: 0.8418\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 13ms/step - loss: 0.6116 - accuracy: 0.8547\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.5718 - accuracy: 0.8672\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.5356 - accuracy: 0.8773\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.5020 - accuracy: 0.8870\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.4725 - accuracy: 0.8950\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}